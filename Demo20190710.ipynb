{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中英文斷詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'a', 'book']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'this is a book'\n",
    "s.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'love', 'this', 'book']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2 = 'i love this book'\n",
    "s2.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['酸民婉君也可以報名嗎?']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = '酸民婉君也可以報名嗎?'\n",
    "s.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jieba\n",
      "  Downloading https://files.pythonhosted.org/packages/71/46/c6f9179f73b818d5827202ad1c4a94e371a29473b7f043b736b4dab6b8cd/jieba-0.39.zip (7.3MB)\n",
      "Building wheels for collected packages: jieba\n",
      "  Running setup.py bdist_wheel for jieba: started\n",
      "  Running setup.py bdist_wheel for jieba: finished with status 'done'\n",
      "  Stored in directory: D:\\Users\\nc20\\AppData\\Local\\pip\\Cache\\wheels\\c9\\c7\\63\\a9ec0322ccc7c365fd51e475942a82395807186e94f0522243\n",
      "Successfully built jieba\n",
      "Installing collected packages: jieba\n",
      "Successfully installed jieba-0.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed 1.21.8 requires msgpack, which is not installed.\n",
      "You are using pip version 10.0.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "! pip install --trusted-host pypi.org --trusted-host files.pythonhosted.org jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache D:\\Users\\nc20\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.431 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "酸民婉君\n",
      "也\n",
      "可以\n",
      "報名\n",
      "嗎\n",
      "?\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "s = '酸民婉君也可以報名嗎?'\n",
    "for l in jieba.cut(s):\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['酸民婉君', '也', '可以', '報名', '嗎', '?']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(jieba.cut(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['林志玲上月閃婚',\n",
       " '不諱言渴望生雙胞胎',\n",
       " '今網路突然瘋傳一組她小腹隆起的照片',\n",
       " '網友紛紛道賀她「有喜」',\n",
       " '但大家恐怕要失望了',\n",
       " '這組照片其實是去年10月的志玲姊姊！']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = '''林志玲上月閃婚，不諱言渴望生雙胞胎，今網路突然瘋傳一組她小腹隆起的照片，網友紛紛道賀她「有喜」，但大家恐怕要失望了，這組照片其實是去年10月的志玲姊姊！'''\n",
    "s.split('，')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['林志玲上月閃婚',\n",
       " '不諱言渴望生雙胞胎',\n",
       " '今網路突然瘋傳一組她小腹隆起的照片',\n",
       " '網友紛紛道賀她',\n",
       " '有喜',\n",
       " '',\n",
       " '但大家恐怕要失望了',\n",
       " '這組照片其實是去年10月的志玲姊姊',\n",
       " '']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.split('「|」|！|，', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['這組', '照片', '其實', '是', '去年', '10', '月', '的', '志玲', '姊姊']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = '這組照片其實是去年10月的志玲姊姊'\n",
    "list(jieba.cut(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jieba 斷詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['這', '組', '照片', '其', '實', '是', '去年', '10', '月', '的', '志', '玲', '姊姊']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(jieba.cut(s, cut_all = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Mode: 大/ 巨蛋/ 案/ 對/ 市府/ 同仁/ 下/ 封口/ 封口令/ 口令/ / / / 柯/ P/ 否/ 認\n"
     ]
    }
   ],
   "source": [
    "seg_list = jieba.cut(\"大巨蛋案對市府同仁下封口令？ 柯P否認\", cut_all=True)\n",
    "print(\"Full Mode:\", \"/ \".join(seg_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Mode: 大/ 巨蛋/ 案對/ 市府/ 同仁/ 下/ 封口令/ ？/  / 柯/ P/ 否認\n"
     ]
    }
   ],
   "source": [
    "seg_list = jieba.cut(\"大巨蛋案對市府同仁下封口令？ 柯P否認\", cut_all=False)\n",
    "print(\"Default Mode:\", \"/ \".join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Mode: 大巨蛋/ 案對/ 市府/ 同仁/ 下/ 封口令/ ？/  / 柯P/ 否認\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "jieba.load_userdict('userdict.txt')\n",
    "seg_list = jieba.cut(\"大巨蛋案對市府同仁下封口令？ 柯P否認\", cut_all=False)\n",
    "print(\"Default Mode:\", \"/ \".join(seg_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jieba 繁體中文版\n",
    "- https://github.com/ldkrsi/jieba-zh_TW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "zip_ref = zipfile.ZipFile('D:/Users/nc20/Downloads/jieba-zh_TW-master.zip', 'r')\n",
    "zip_ref.extractall('jieba_zh_tw')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Mode: 大巨蛋/ 案對/ 市府/ 同仁/ 下/ 封口令/ ？/  / 柯P/ 否認\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "jieba.load_userdict('userdict.txt')\n",
    "seg_list = jieba.cut(\"大巨蛋案對市府同仁下封口令？ 柯P否認\", cut_all=False)\n",
    "print(\"Default Mode:\", \"/ \".join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大巨蛋 N\n",
      "案 ng\n",
      "對 p\n",
      "市府 N\n",
      "同仁 N\n",
      "下 POST\n",
      "封口令 n\n",
      "？ x\n",
      "  x\n",
      "柯P N\n",
      "否認 v\n"
     ]
    }
   ],
   "source": [
    "import jieba.posseg as pseg\n",
    "jieba.load_userdict('userdict.txt')\n",
    "words = pseg.cut(\"大巨蛋案對市府同仁下封口令？ 柯P否認\")\n",
    "for w in words:\n",
    "    print(w.word, w.flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大巨蛋 0 3\n",
      "案對 3 5\n",
      "市府 5 7\n",
      "同仁 7 9\n",
      "下 9 10\n",
      "封口令 10 13\n",
      "？ 13 14\n",
      "  14 15\n",
      "柯P 15 17\n",
      "否認 17 19\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "sentence = \"大巨蛋案對市府同仁下封口令？ 柯P否認\"\n",
    "words = jieba.tokenize(sentence)\n",
    "for tw in words:\n",
    "    print(tw[0], tw[1], tw[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = '''\n",
    "［記者黃旭磊／高雄報導］高市新增1例本土登革熱病例，50多歲婦為第38例患者，發病前曾至幼兒園接送孫子，所幸同住4人並未發生疑似症狀。\n",
    "\n",
    "高雄市衛生局今天傍晚公布新增病例，三民區寶民里50多歲婦檢驗分型為第4型，患者過去未罹患過登革熱，平時至寶業滯洪公園散步、澄和黃昏市場採買並接送孫子至幼兒園，所幸家人均無疑似症狀。\n",
    "\n",
    "衛生局副局長林盟喬說，市區確診38例本土登革熱，其中，32例居住於三民區（鼎金里16例、本和里5例、鼎西里4例、鼎強里及安吉里各2例、鼎力里、鼎盛里及寶民里各1例）、1例於前鎮、1例於鳳山、1例於鼓山、1例於路竹、2例於左營菜公里，至於境外確診登革熱病例33例。\n",
    "\n",
    "病例發生後，防疫團隊至寶民里社區進行採血作業，今晚9點半將於澄和市場噴藥，而疫情至今感染熱區多在三民區，包括金獅湖一帶（24例）、本和里（5例）及安吉里（2例），至於左營區菜公里也有2例，當地自由黃昏市場封閉3天噴藥，已重新開放。\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "jieba.load_userdict('userdict.txt')\n",
    "words = list(jieba.cut(news))\n",
    "#words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {}\n",
    "for w in words:\n",
    "    if w not in dic:\n",
    "        dic[w] = 1\n",
    "    else:\n",
    "        dic[w] = dic[w] + 1\n",
    "#dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dic.items()\n",
    "swd = sorted(dic.items(), key=lambda e:e[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "病例 4\n",
      "登革熱 3\n",
      "三民區 3\n",
      "寶民里 3\n",
      "新增 2\n",
      "本土 2\n",
      "50 2\n",
      "多歲 2\n",
      "38 2\n",
      "患者 2\n",
      "幼兒園 2\n",
      "接送 2\n",
      "孫子 2\n",
      "所幸 2\n"
     ]
    }
   ],
   "source": [
    "for k,v in swd[0:30]:\n",
    "    if len(k) >=2:\n",
    "        print(k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counter\n",
    "- https://docs.python.org/3.7/library/collections.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 5), (3, 3), (1, 2), (5, 2), (4, 1), (6, 1)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,2,2,3,3,2,1,2,3,4,5,5,6]\n",
    "c = Counter(a)\n",
    "c.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "病例 4\n",
      "登革熱 3\n",
      "三民區 3\n",
      "寶民里 3\n",
      "新增 2\n",
      "本土 2\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "from collections import Counter\n",
    "jieba.load_userdict('userdict.txt')\n",
    "words = list(jieba.cut(news))\n",
    "c = Counter(words)\n",
    "for k,v in c.most_common(20):\n",
    "    if len(k) >=2 :\n",
    "        print(k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, abb, abc = [\"a\"], [\"a\", \"b\", \"b\"], [\"a\", \"b\", \"c\"]\n",
    "D = [a, abb, abc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "#tfidf('a', a, D)\n",
    "tf = 1/1\n",
    "idf = math.log(3/3)\n",
    "tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tfidf('a', abb, D)\n",
    "tf = 1/3\n",
    "idf = math.log(3/3)\n",
    "tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tfidf('a', abc, D)\n",
    "tf = 1/3\n",
    "idf = math.log(3/3)\n",
    "tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27031007207210955"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tfidf('b', abb, D)\n",
    "tf = 2/3\n",
    "idf = math.log(3/2)\n",
    "tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13515503603605478"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tfidf('b', abc, D)\n",
    "tf = 1/3\n",
    "idf = math.log(3/2)\n",
    "tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3662040962227032"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tfidf('c', abc, D)\n",
    "tf = 1/3\n",
    "idf = math.log(3/1)\n",
    "tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ['a', 'b', 'a', 'c', 'a']\n",
    "a.count('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D\n",
    "ary = []\n",
    "for doc in D:\n",
    "    if 'b' in doc:\n",
    "        ary.append(doc)\n",
    "len(ary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(t, d, D):\n",
    "    tf = d.count(t) / len(d)\n",
    "    idf = math.log(len(D)/ len([doc for doc in D if t in doc]))\n",
    "    return tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27031007207210955"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf('b', abb, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13515503603605478"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf('b', abc, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3662040962227032"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf('c', abc, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba.analyse\n",
    "tags = jieba.analyse.extract_tags(news, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['登革熱', '三民區', '寶民里']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 詞頻矩陣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "content = [\"How to format my hard disk\", \" Hard disk format problems \"]\n",
    "X = vectorizer.fit_transform(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['disk', 'format', 'hard', 'how', 'my', 'problems', 'to']\n",
      "[[1 1 1 1 1 0 1]\n",
      " [1 1 1 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  1,  1, -1,  1], dtype=int64)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = X.toarray()\n",
    "s[0] - s[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = ['This is a toy post about machine learning. Actually, it contains not much interesting stuff.',\n",
    "'Imaging databases can get huge.',\n",
    "'Most imaging databases safe images permanently.',\n",
    "'Imaging databases store images.',\n",
    "'Imaging databases store images. Imaging databases store images. Imaging databases store images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#?CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "\n",
    "X_train = vectorizer.fit_transform(posts)\n",
    "num_samples, num_features = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['about',\n",
       " 'actually',\n",
       " 'can',\n",
       " 'contains',\n",
       " 'databases',\n",
       " 'get',\n",
       " 'huge',\n",
       " 'images',\n",
       " 'imaging',\n",
       " 'interesting',\n",
       " 'is',\n",
       " 'it',\n",
       " 'learning',\n",
       " 'machine',\n",
       " 'most',\n",
       " 'much',\n",
       " 'not',\n",
       " 'permanently',\n",
       " 'post',\n",
       " 'safe',\n",
       " 'store',\n",
       " 'stuff',\n",
       " 'this',\n",
       " 'toy']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,\n",
       "        1, 1],\n",
       "       [0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 3, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0,\n",
       "        0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_post = 'imaging database'\n",
    "new_post_vec = vectorizer.transform([new_post])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_post_vec.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "def dist_raw(v1, v2):\n",
    "    delta = v1-v2\n",
    "    return sp.linalg.norm(delta.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.872983346207417"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_raw(new_post_vec, X_train.getrow(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "3.872983346207417\n",
      "=====================\n",
      "Imaging databases can get huge.\n",
      "2.0\n",
      "=====================\n",
      "Most imaging databases safe images permanently.\n",
      "2.23606797749979\n",
      "=====================\n",
      "Imaging databases store images.\n",
      "1.7320508075688772\n",
      "=====================\n",
      "Imaging databases store images. Imaging databases store images. Imaging databases store images\n",
      "5.5677643628300215\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "for i in range(X_train.shape[0]):\n",
    "    print(posts[i])\n",
    "    print(dist_raw(new_post_vec, X_train.getrow(i)))\n",
    "    print('=====================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(v1, v2):\n",
    "    v1_normalized = v1 / sp.linalg.norm(v1.toarray())\n",
    "    v2_normalized = v2 / sp.linalg.norm(v2.toarray())\n",
    "    delta = v1_normalized - v2_normalized\n",
    "    return sp.linalg.norm(delta.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "1.414213562373095\n",
      "=====================\n",
      "Imaging databases can get huge.\n",
      "1.0514622242382672\n",
      "=====================\n",
      "Most imaging databases safe images permanently.\n",
      "1.0878894332937856\n",
      "=====================\n",
      "Imaging databases store images.\n",
      "1.0\n",
      "=====================\n",
      "Imaging databases store images. Imaging databases store images. Imaging databases store images\n",
      "1.0\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "for i in range(X_train.shape[0]):\n",
    "    print(posts[i])\n",
    "    print(dist(new_post_vec, X_train.getrow(i)))\n",
    "    print('=====================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words = 'english')\n",
    "X_train = vectorizer.fit_transform(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_post = 'imaging database'\n",
    "new_post_vec = vectorizer.transform([new_post])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(v1, v2):\n",
    "    v1_normalized = v1 / sp.linalg.norm(v1.toarray())\n",
    "    v2_normalized = v2 / sp.linalg.norm(v2.toarray())\n",
    "    delta = v1_normalized - v2_normalized\n",
    "    return sp.linalg.norm(delta.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "1.4142135623730951\n",
      "=====================\n",
      "Imaging databases can get huge.\n",
      "0.9194016867619662\n",
      "=====================\n",
      "Most imaging databases safe images permanently.\n",
      "1.0514622242382672\n",
      "=====================\n",
      "Imaging databases store images.\n",
      "1.0\n",
      "=====================\n",
      "Imaging databases store images. Imaging databases store images. Imaging databases store images\n",
      "1.0\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "for i in range(X_train.shape[0]):\n",
    "    print(posts[i])\n",
    "    print(dist(new_post_vec, X_train.getrow(i)))\n",
    "    print('=====================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'fifty', 'now', 'un', 'more', 'empty', 'were', 'get', 'themselves', 'few', 'since', 'up', 'due', 'latterly', 'name', 'other', 'us', 'rather', 'that', 'no', 'too', 'though', 'well', 'beside', 'to', 'everywhere', 'put', 'either', 'every', 'detail', 'cant', 'first', 'own', 'before', 'back', 'last', 'ours', 'see', 'hers', 'above', 'between', 'its', 'after', 'almost', 'be', 'seem', 'fifteen', 'these', 'he', 'his', 'thru', 'inc', 'latter', 'perhaps', 'co', 'six', 'bottom', 'against', 'it', 'could', 'next', 'serious', 'where', 'may', 'only', 'therein', 'so', 'would', 'about', 'done', 'whither', 'was', 'under', 'who', 'show', 'they', 're', 'whenever', 'of', 'call', 'whereafter', 'which', 'has', 'we', 'wherein', 'down', 'out', 'top', 'please', 'than', 'do', 'namely', 'whoever', 'besides', 'an', 'less', 'nor', 'yourself', 'whatever', 'nine', 'himself', 'she', 'still', 'should', 'whence', 'same', 'anyone', 'formerly', 'while', 'much', 'whereas', 'off', 'during', 'twelve', 'by', 'a', 'any', 'each', 'anywhere', 'her', 'otherwise', 'thick', 'them', 'am', 'as', 'de', 'front', 'in', 'found', 'me', 'eight', 'ever', 'again', 'hereafter', 'herself', 'everyone', 'onto', 'side', 'those', 'already', 'also', 'mine', 'myself', 'one', 'amount', 'might', 'twenty', 'through', 'for', 'your', 'hence', 'others', 'will', 'indeed', 'being', 'con', 'go', 'thus', 'mill', 'why', 'at', 'several', 'can', 'becomes', 'within', 'hereby', 'made', 'their', 'forty', 'most', 'fire', 'this', 'ten', 'many', 'whom', 'often', 'via', 'have', 'together', 'hundred', 'when', 'across', 'alone', 'nothing', 'along', 'because', 'seeming', 'if', 'anyhow', 'into', 'cry', 'you', 'although', 'herein', 'always', 'therefore', 'seemed', 'all', 'and', 'else', 'once', 'without', 'etc', 'five', 'around', 'describe', 'sometime', 'whose', 'with', 'anyway', 'sometimes', 'not', 'throughout', 'both', 'what', 'another', 'ltd', 'amoungst', 'whether', 'became', 'him', 'sixty', 'thereby', 'even', 'must', 'except', 'sincere', 'is', 'on', 'something', 'there', 'thereupon', 'three', 'very', 'whole', 'further', 'among', 'itself', 'keep', 'ourselves', 'here', 'yours', 'enough', 'are', 'over', 'the', 'full', 'becoming', 'afterwards', 'mostly', 'such', 'four', 'move', 'moreover', 'wherever', 'system', 'nowhere', 'third', 'been', 'elsewhere', 'least', 'never', 'noone', 'eg', 'become', 'whereupon', 'bill', 'until', 'part', 'cannot', 'from', 'two', 'somewhere', 'our', 'anything', 'everything', 'neither', 'thereafter', 'towards', 'thence', 'my', 'nevertheless', 'somehow', 'behind', 'take', 'upon', 'seems', 'someone', 'yet', 'how', 'below', 'toward', 'find', 'whereby', 'beforehand', 'couldnt', 'per', 'fill', 'interest', 'or', 'hasnt', 'some', 'however', 'then', 'thin', 'but', 'eleven', 'had', 'nobody', 'none', 'beyond', 'give', 'amongst', 'hereupon', 'yourselves', 'meanwhile', 'ie', 'i', 'former'})\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_stop_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.3)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.11.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed 1.21.8 requires msgpack, which is not installed.\n",
      "You are using pip version 10.0.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "! pip install --trusted-host pypi.org --trusted-host files.pythonhosted.org nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graphic'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk.stem\n",
    "s = nltk.stem.SnowballStemmer('english')\n",
    "s.stem('graphics')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imag\n",
      "imag\n",
      "imagin\n",
      "imagin\n"
     ]
    }
   ],
   "source": [
    "print(s.stem(\"imaging\"))\n",
    "print(s.stem(\"image\"))\n",
    "print(s.stem(\"imagination\"))\n",
    "print(s.stem(\"imagine\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.stem\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    \n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#?CountVectorizer\n",
    "vectorizer = StemmedCountVectorizer(min_df=1)\n",
    "\n",
    "X_train = vectorizer.fit_transform(posts)\n",
    "num_samples, num_features = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_post = 'imaging database'\n",
    "new_post_vec = vectorizer.transform([new_post])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about', 'actual', 'can', 'contain', 'databas', 'get', 'huge', 'imag', 'interest', 'is', 'it', 'learn', 'machin', 'most', 'much', 'not', 'perman', 'post', 'safe', 'store', 'stuff', 'this', 'toy']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0]], dtype=int64)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_post_vec.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(v1, v2):\n",
    "    v1_normalized = v1 / sp.linalg.norm(v1.toarray())\n",
    "    v2_normalized = v2 / sp.linalg.norm(v2.toarray())\n",
    "    delta = v1_normalized - v2_normalized\n",
    "    return sp.linalg.norm(delta.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "1.414213562373095\n",
      "=====================\n",
      "Imaging databases can get huge.\n",
      "0.8573732768944039\n",
      "=====================\n",
      "Most imaging databases safe images permanently.\n",
      "0.7071067811865475\n",
      "=====================\n",
      "Imaging databases store images.\n",
      "0.5176380902050415\n",
      "=====================\n",
      "Imaging databases store images. Imaging databases store images. Imaging databases store images\n",
      "0.5176380902050415\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "for i in range(X_train.shape[0]):\n",
    "    print(posts[i])\n",
    "    print(dist(new_post_vec, X_train.getrow(i)))\n",
    "    print('=====================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "\n",
    "X_train = vectorizer.fit_transform(posts)\n",
    "num_samples, num_features = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.26726124, 0.26726124, 0.        , 0.26726124, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.26726124,\n",
       "        0.26726124, 0.26726124, 0.26726124, 0.26726124, 0.        ,\n",
       "        0.26726124, 0.26726124, 0.        , 0.26726124, 0.        ,\n",
       "        0.        , 0.26726124, 0.26726124, 0.26726124],\n",
       "       [0.        , 0.        , 0.52451722, 0.        , 0.29550385,\n",
       "        0.52451722, 0.52451722, 0.        , 0.29550385, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.27880274,\n",
       "        0.        , 0.        , 0.33142212, 0.27880274, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.49487286,\n",
       "        0.        , 0.        , 0.49487286, 0.        , 0.49487286,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.42780918,\n",
       "        0.        , 0.        , 0.50855106, 0.42780918, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.61264544, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.42780918,\n",
       "        0.        , 0.        , 0.50855106, 0.42780918, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.61264544, 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_post = 'imaging database'\n",
    "new_post_vec = vectorizer.transform([new_post])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "1.414213562373095\n",
      "=====================\n",
      "Imaging databases can get huge.\n",
      "1.187009812033225\n",
      "=====================\n",
      "Most imaging databases safe images permanently.\n",
      "1.2009973044062707\n",
      "=====================\n",
      "Imaging databases store images.\n",
      "1.0697577475609328\n",
      "=====================\n",
      "Imaging databases store images. Imaging databases store images. Imaging databases store images\n",
      "1.0697577475609328\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "for i in range(X_train.shape[0]):\n",
    "    print(posts[i])\n",
    "    print(dist(new_post_vec, X_train.getrow(i)))\n",
    "    print('=====================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer,self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "vectorizer = StemmedTfidfVectorizer(min_df=1)\n",
    "\n",
    "X_train = vectorizer.fit_transform(posts)\n",
    "num_samples, num_features = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_post = 'imaging database'\n",
    "new_post_vec = vectorizer.transform([new_post])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "1.4142135623730951\n",
      "=====================\n",
      "Imaging databases can get huge.\n",
      "1.0789758507558254\n",
      "=====================\n",
      "Most imaging databases safe images permanently.\n",
      "0.9401975637779192\n",
      "=====================\n",
      "Imaging databases store images.\n",
      "0.634205801303706\n",
      "=====================\n",
      "Imaging databases store images. Imaging databases store images. Imaging databases store images\n",
      "0.634205801303706\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "for i in range(X_train.shape[0]):\n",
    "    print(posts[i])\n",
    "    print(dist(new_post_vec, X_train.getrow(i)))\n",
    "    print('=====================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['柯文哲 為 了 大巨蛋 一事 找 趙藤雄 算帳', '柯P 將不在 大巨蛋 舉辦 世運會']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "jieba.load_userdict('userdict.txt')\n",
    "a = ['柯文哲為了大巨蛋一事找趙藤雄算帳','柯P將不在大巨蛋舉辦世運會']\n",
    "corpus = [' '.join(jieba.cut(s)) for s in a]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x9 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 10 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['一事', '世運會', '大巨蛋', '將不在', '柯p', '柯文哲', '算帳', '舉辦', '趙藤雄']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0, 0, 1, 1, 0, 1],\n",
       "       [0, 1, 1, 1, 1, 0, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2649110640673518"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist(X[0], X[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 處理同義詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'柯文哲/柯P/KP'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "term ='柯文哲'\n",
    "res = requests.get('https://zh.wikipedia.org/wiki/{}'.format(term))\n",
    "soup = BeautifulSoup(res.text,'lxml')\n",
    "'/'.join([w.text for w in soup.select('.mw-parser-output p')[5].select('b')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'柯p': '柯文哲', 'kp': '柯文哲'}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonym_dic = {}\n",
    "for s in open('synonym.txt', encoding='utf-8'):\n",
    "    synonym = s.strip().split('/')\n",
    "    for w in synonym[1:]:\n",
    "        synonym_dic[w.lower()] = synonym[0]\n",
    "synonym_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynonymCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(SynonymCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (synonym_dic.get(w, w) for w in analyzer(doc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['一事', '世運會', '大巨蛋', '將不在', '柯文哲', '算帳', '舉辦', '趙藤雄']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = SynonymCountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0, 1, 1, 0, 1],\n",
       "       [0, 1, 1, 1, 1, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0954451150103321"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist(X[0], X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['為了','一事','將不在']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['世運會', '大巨蛋', '柯文哲', '算帳', '舉辦', '趙藤雄']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = SynonymCountVectorizer(stop_words=stopwords)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 1],\n",
       "       [1, 1, 1, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist(X[0], X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
